{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(1, os.path.abspath(\"../../../\"))  # for dowhy source code\n",
    "sys.path.insert(2, os.path.abspath(\"../../../../EconML/\")) # for econml source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "import dowhy\n",
    "from dowhy import CausalModel\n",
    "import dowhy.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>W0</th>\n",
       "      <th>W1</th>\n",
       "      <th>W2</th>\n",
       "      <th>W3</th>\n",
       "      <th>v</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.355812</td>\n",
       "      <td>0.081412</td>\n",
       "      <td>0.795749</td>\n",
       "      <td>-0.131001</td>\n",
       "      <td>-2.262737</td>\n",
       "      <td>0.684836</td>\n",
       "      <td>-3.718250</td>\n",
       "      <td>-45.475559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.465739</td>\n",
       "      <td>-1.517709</td>\n",
       "      <td>-0.727935</td>\n",
       "      <td>-1.878542</td>\n",
       "      <td>0.451740</td>\n",
       "      <td>-0.428575</td>\n",
       "      <td>-10.526795</td>\n",
       "      <td>-109.481331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.830387</td>\n",
       "      <td>-0.705246</td>\n",
       "      <td>0.810064</td>\n",
       "      <td>-0.639895</td>\n",
       "      <td>-2.353308</td>\n",
       "      <td>-0.303040</td>\n",
       "      <td>-8.222476</td>\n",
       "      <td>-94.187557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.200510</td>\n",
       "      <td>-0.184273</td>\n",
       "      <td>-1.055428</td>\n",
       "      <td>0.120913</td>\n",
       "      <td>0.908116</td>\n",
       "      <td>-0.793236</td>\n",
       "      <td>-2.417047</td>\n",
       "      <td>-22.612623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.183300</td>\n",
       "      <td>-1.356460</td>\n",
       "      <td>-0.876159</td>\n",
       "      <td>-1.810650</td>\n",
       "      <td>-1.736931</td>\n",
       "      <td>-0.807156</td>\n",
       "      <td>-19.792463</td>\n",
       "      <td>-213.371791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X0        X1        W0        W1        W2        W3          v  \\\n",
       "0 -0.355812  0.081412  0.795749 -0.131001 -2.262737  0.684836  -3.718250   \n",
       "1  1.465739 -1.517709 -0.727935 -1.878542  0.451740 -0.428575 -10.526795   \n",
       "2  0.830387 -0.705246  0.810064 -0.639895 -2.353308 -0.303040  -8.222476   \n",
       "3  0.200510 -0.184273 -1.055428  0.120913  0.908116 -0.793236  -2.417047   \n",
       "4  0.183300 -1.356460 -0.876159 -1.810650 -1.736931 -0.807156 -19.792463   \n",
       "\n",
       "            y  \n",
       "0  -45.475559  \n",
       "1 -109.481331  \n",
       "2  -94.187557  \n",
       "3  -22.612623  \n",
       "4 -213.371791  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dowhy.datasets.linear_dataset(10, num_common_causes=4, num_samples=10000,\n",
    "                                    num_instruments=0, num_effect_modifiers=2,\n",
    "                                    treatment_is_binary=False)\n",
    "df=data['df']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dowhy.causal_model:Model to find the causal effect of treatment ['v'] on outcome ['y']\n"
     ]
    }
   ],
   "source": [
    "model = CausalModel(data=data[\"df\"], \n",
    "                    treatment=data[\"treatment_name\"], outcome=data[\"outcome_name\"], \n",
    "                    graph=data[\"gml_graph\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dowhy.causal_identifier:Common causes of treatment and outcome:['W3', 'W1', 'W2', 'Unobserved Confounders', 'W0']\n",
      "WARNING:dowhy.causal_identifier:There are unobserved common causes. Causal effect cannot be identified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Do you want to continue by ignoring these unobserved confounders? [y/n] y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dowhy.causal_identifier:Instrumental variables for treatment and outcome:[]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimand type: ate\n",
      "### Estimand : 1\n",
      "Estimand name: backdoor\n",
      "Estimand expression:\n",
      "d                             \n",
      "â”€â”€(Expectation(y|W3,W1,W2,W0))\n",
      "dv                            \n",
      "Estimand assumption 1, Unconfoundedness: If Uâ†’v and Uâ†’y then P(y|v,W3,W1,W2,W0,U) = P(y|v,W3,W1,W2,W0)\n",
      "### Estimand : 2\n",
      "Estimand name: iv\n",
      "No such variable found!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "identified_estimand= model.identify_effect()\n",
    "print(identified_estimand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dowhy.causal_estimator:INFO: Using EconML Estimator\n",
      "INFO:dowhy.causal_estimator:b: y~v+W3+W1+W2+W0\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/gradient_boosting.py:1450: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/gradient_boosting.py:1450: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/gradient_boosting.py:1450: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/ensemble/gradient_boosting.py:1450: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/coordinate_descent.py:1100: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/mnt/c/Users/amit_/code/EconML/econml/dml.py:245: UserWarning: The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'DMLCateEstimator' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-71641bb4d9b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m estimate = model.estimate_effect(identified_estimand, method_name=\"backdoor.econml.dml.DMLCate\",\n\u001b[0;32m----> 5\u001b[0;31m                                 method_params={\"init_params\":{'model_y':GradientBoostingRegressor(),'model_t': GradientBoostingRegressor(),\"model_final\":LassoCV(), 'featurizer':PolynomialFeatures(degree=1, include_bias=True)}, \"fit_params\":{}})\n\u001b[0m",
      "\u001b[0;32m/mnt/c/Users/amit_/code/dowhy/dowhy/causal_model.py\u001b[0m in \u001b[0;36mestimate_effect\u001b[0;34m(self, identified_estimand, method_name, test_significance, evaluate_effect_strength, target_units, effect_modifiers, method_params)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             )\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mestimate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcausal_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_effect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             estimate.add_params(\n\u001b[1;32m    179\u001b[0m                 \u001b[0mestimand_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midentified_estimand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimand_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/amit_/code/dowhy/dowhy/causal_estimator.py\u001b[0m in \u001b[0;36mestimate_effect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_significance_test_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignif_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_effect_strength_eval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0meffect_strength_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_effect_strength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_effect_strength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meffect_strength_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/amit_/code/dowhy/dowhy/causal_estimator.py\u001b[0m in \u001b[0;36mevaluate_effect_strength\u001b[0;34m(self, estimate)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_effect_strength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mfraction_effect_explained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate_effect_strength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fraction-effect\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Need to test r-squared before supporting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m#effect_r_squared = self._evaluate_effect_strength(estimate, method=\"r-squared\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/amit_/code/dowhy/dowhy/causal_estimator.py\u001b[0m in \u001b[0;36m_evaluate_effect_strength\u001b[0;34m(self, estimate, method)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fraction-effect\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mnaive_obs_estimate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_effect_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mfraction_effect_explained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnaive_obs_estimate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfraction_effect_explained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m#elif method == \"r-squared\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'DMLCateEstimator' and 'float'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "estimate = model.estimate_effect(identified_estimand, method_name=\"backdoor.econml.dml.DMLCate\",\n",
    "                                method_params={\"init_params\":{'model_y':GradientBoostingRegressor(),'model_t': GradientBoostingRegressor(),\"model_final\":LassoCV(), 'featurizer':PolynomialFeatures(degree=1, include_bias=True)}, \"fit_params\":{}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import econml\n",
    "from econml.dml import DMLCateEstimator, LinearDMLCateEstimator, SparseLinearDMLCateEstimator\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from econml.bootstrap import BootstrapEstimator\n",
    "from sklearn.linear_model import MultiTaskElasticNet\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Instance parameters\n",
    "n_controls = 2\n",
    "n_instruments = 1\n",
    "n_features = 1\n",
    "n_treatments = 1\n",
    "alpha = np.random.normal(size=(n_controls, 1))\n",
    "beta = np.random.normal(size=(n_instruments, 1))\n",
    "gamma = np.random.normal(size=(n_treatments, 1))\n",
    "delta = np.random.normal(size=(n_treatments, 1))\n",
    "zeta = np.random.normal(size=(n_controls, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Data\n",
    "n_samples = 10000\n",
    "W = np.random.normal(size=(n_samples, n_controls)) # confounders\n",
    "Z = np.random.normal(size=(n_samples, n_instruments)) # instrumental variables\n",
    "X = np.random.normal(size=(n_samples, n_features)) # effect modifiers\n",
    "eta = np.random.normal(size=(n_samples, n_treatments)) \n",
    "epsilon = np.random.normal(size=(n_samples, 1))\n",
    "T = np.dot(W, alpha) + np.dot(Z, beta) + eta # treatment\n",
    "y = np.dot(T**2, gamma) + np.dot(np.multiply(T, X), delta) + np.dot(W, zeta) + epsilon\n",
    "print(gamma, delta) # causal effect parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit counterfactual model\n",
    "est = LinearDMLCateEstimator()\n",
    "    #model_y=MultiTaskElasticNet(alpha=0.1),\n",
    "    #                model_t=MultiTaskElasticNet(alpha=0.1),\n",
    "    #                featurizer=PolynomialFeatures(degree=1, include_bias=False))\n",
    "print(W, W.shape)\n",
    "print(np.ndarray(shape=(n_samples,2), buffer=np.array(df[['X0','X1']])), df['X0'].shape)\n",
    "est.fit(\n",
    "    np.ndarray(shape=(n_samples,1), buffer=np.array(df['y'])),\n",
    "    np.ndarray(shape=(n_samples,1), buffer=np.array(df['v'])),\n",
    "    X=np.ndarray(shape=(n_samples,1), buffer=np.array(df[['X0', 'X1']])),\n",
    "    W=np.ndarray(shape=(n_samples,1), buffer=np.array(df['W0'])),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X\n",
    "# Estimate heterogeneous treatment effects from going from treatment 0 to treatment 1\n",
    "T0_test = np.zeros((X_test.shape[0], n_treatments))\n",
    "T1_test = np.ones((X_test.shape[0], n_treatments))\n",
    "#hetero_te = cfest.effect(T0_test, T1_test, X_test)\n",
    "te_pred = est.const_marginal_effect(np.array([[np.median(X)]]))\n",
    "print(te_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X\n",
    "# Estimate heterogeneous treatment effects from going from treatment 0 to treatment 1\n",
    "T0_test = np.zeros((X_test.shape[0], n_treatments))\n",
    "T1_test = np.ones((X_test.shape[0], n_treatments))\n",
    "print(est.effect(X, T0=T0_test, T1=T1_test)[:5])\n",
    "# To get the coefficients of the polynomial fitted in the final stage we can\n",
    "# access the coef_ attribute of the fitted second stage model. This would\n",
    "# return the coefficients in front of each term in the vector TâŠ—Ï•(X).\n",
    "a_hat = est.coef_\n",
    "y0 = np.dot(T0_test**2, gamma) + np.dot(np.multiply(T0_test, X), delta) + np.dot(W, zeta) + epsilon\n",
    "y1 = np.dot(T1_test**2, gamma) + np.dot(np.multiply(T1_test, X), delta) + np.dot(W, zeta) + epsilon\n",
    "tau = y1-y0\n",
    "print(tau[:5])\n",
    "print(a_hat) \n",
    "print(gamma, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate heterogeneous marginal effects around treatment 0\n",
    "T_test = np.zeros((X_test.shape[0], n_treatments))\n",
    "hetero_marginal_te = est.marginal_effect(T_test, X_test)\n",
    "print(y[1])\n",
    "hetero_marginal_te[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate average treatment effects over a population of z's\n",
    "T0_test = np.zeros((X_test.shape[0], n_treatments))\n",
    "T1_test = np.ones((X_test.shape[0], n_treatments))\n",
    "\n",
    "# average treatment effect\n",
    "ate = np.mean(est.effect(X_test, T0=T0_test, T1=T1_test)) # returns estimate of Î³ + Î´ ð”¼[x]\n",
    "np.ndarray(shape=(10,1), buffer=X_test[[X_test>1]])\n",
    "# average treatment effect of population with x>1/2\n",
    "# returns estimate of Î³ + Î´ ð”¼[x | x>1/2]\n",
    "#cate = np.mean(est.effect(X_test[X_test>1/2], T0=T0_test[X_test>1/2], T1=T1_test[X_test>1/2] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate expected lift of treatment policy: Ï€(z) = ðŸ™{x > 0} over existing policy\n",
    "Pi0_test = T\n",
    "Pi1_test = (X_test > 0) * 1.\n",
    "# returns estimate of Î³/2 + Î´/âˆš(2Ï€)\n",
    "policy_effect = np.mean(est.effect(Pi0_test, Pi1_test, X_test))\n",
    "\n",
    "# Estimate expected lift of treatment policy: Ï€(x) = ðŸ™{x > 0} over baseline of no treatment\n",
    "Pi0_test = np.zeros((X_test.shape[0], n_treatments))\n",
    "Pi1_test = (X_test > 0) * 1.\n",
    "# returns estimate of Î³/2 + Î´/âˆš(2Ï€)\n",
    "policy_effect = np.mean(est.effect(Pi0_test, Pi1_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dowhy.datasets.linear_dataset(beta=10,\n",
    "        num_common_causes=5, \n",
    "        num_instruments = 2,\n",
    "        num_samples=10000,\n",
    "        treatment_is_binary=True)\n",
    "df = data[\"df\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.dml import DMLCateEstimator\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "est = DMLCateEstimator(model_y=LassoCV(), model_t=LassoCV())\n",
    "est.fit(df[\"y\"], df[\"v\"], df[\"Z0\"], df[[\"X1\", \"X2\"]]) # W -> high-dimensional confounders, X -> features\n",
    "treatment_effects = est.effect(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
